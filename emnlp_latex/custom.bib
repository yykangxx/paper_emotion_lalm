% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").


@inproceedings{zou2022speech,
  title={Speech emotion recognition with co-attention based multi-level acoustic information},
  author={Zou, Heqing and Si, Yuke and Chen, Chen and Rajan, Deepu and Chng, Eng Siong},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7367--7371},
  year={2022},
  organization={IEEE}
}

@misc{plutchik1982psychoevolutionary,
  title={A psychoevolutionary theory of emotions},
  author={Plutchik, Robert},
  year={1982},
  publisher={Sage Publications}
}


@inproceedings{chen2023dst,
  title={Dst: Deformable speech transformer for emotion recognition},
  author={Chen, Weidong and Xing, Xiaofen and Xu, Xiangmin and Pang, Jianxin and Du, Lan},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@misc{li2024multiscaletemporaltransformerspeech,
      title={Multi-Scale Temporal Transformer For Speech Emotion Recognition}, 
      author={Zhipeng Li and Xiaofen Xing and Yuanbo Fang and Weibin Zhang and Hengsheng Fan and Xiangmin Xu},
      year={2024},
      eprint={2410.00390},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2410.00390}, 
}


@article{hsu2021hubert,
  title={Hubert: Self-supervised speech representation learning by masked prediction of hidden units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM transactions on audio, speech, and language processing},
  volume={29},
  pages={3451--3460},
  year={2021},
  publisher={IEEE}
}

@article{chen2022wavlm,
  title={Wavlm: Large-scale self-supervised pre-training for full stack speech processing},
  author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and others},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={16},
  number={6},
  pages={1505--1518},
  year={2022},
  publisher={IEEE}
}

@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}
@inproceedings{baevski2023efficient,
  title={Efficient self-supervised learning with contextualized target representations for vision, speech and language},
  author={Baevski, Alexei and Babu, Arun and Hsu, Wei-Ning and Auli, Michael},
  booktitle={International conference on machine learning},
  pages={1416--1429},
  year={2023},
  organization={PMLR}
}
@inproceedings{morais2022speech,
  title={Speech emotion recognition using self-supervised features},
  author={Morais, Edmilson and Hoory, Ron and Zhu, Weizhong and Gat, Itai and Damasceno, Matheus and Aronowitz, Hagai},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6922--6926},
  year={2022},
  organization={IEEE}
}

@inproceedings{chen2023exploring,
  title={Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition},
  author={Chen, Li-Wei and Rudnicky, Alexander},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}
@inproceedings{li2023exploration,
  title={Exploration of a self-supervised speech model: A study on emotional corpora},
  author={Li, Yuanchao and Mohamied, Yumnah and Bell, Peter and Lai, Catherine},
  booktitle={2022 IEEE Spoken Language Technology Workshop (SLT)},
  pages={868--875},
  year={2023},
  organization={IEEE}
}
@article{chu2024qwen2,
  title={Qwen2-audio technical report},
  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2407.10759},
  year={2024}
}
@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}
@article{team2025kimi,
  title={Kimi k1. 5: Scaling reinforcement learning with llms},
  author={Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others},
  journal={arXiv preprint arXiv:2501.12599},
  year={2025}
}
@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}
@article{kong2024audio,
  title={Audio flamingo: A novel audio language model with few-shot learning and dialogue abilities},
  author={Kong, Zhifeng and Goel, Arushi and Badlani, Rohan and Ping, Wei and Valle, Rafael and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2402.01831},
  year={2024}
}
@article{tang2023salmonn,
  title={Salmonn: Towards generic hearing abilities for large language models},
  author={Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao},
  journal={arXiv preprint arXiv:2310.13289},
  year={2023}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53728--53741},
  year={2023}
}
@article{yuan2023scaling,
  title={Scaling relationship on learning mathematical reasoning with large language models},
  author={Yuan, Zheng and Yuan, Hongyi and Li, Chengpeng and Dong, Guanting and Lu, Keming and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.01825},
  year={2023}
}

@article{sane2025hybrid,
  title={Hybrid Group Relative Policy Optimization: A Multi-Sample Approach to Enhancing Policy Optimization},
  author={Sane, Soham},
  journal={arXiv preprint arXiv:2502.01652},
  year={2025}
}
@article{ma2025audio,
  title={Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language Model},
  author={Ma, Ziyang and Chen, Zhuo and Wang, Yuping and Chng, Eng Siong and Chen, Xie},
  journal={arXiv preprint arXiv:2501.07246},
  year={2025}
}
@article{xie2025audio,
  title={Audio-reasoner: Improving reasoning capability in large audio language models},
  author={Xie, Zhifei and Lin, Mingbao and Liu, Zihang and Wu, Pengcheng and Yan, Shuicheng and Miao, Chunyan},
  journal={arXiv preprint arXiv:2503.02318},
  year={2025}
}
@article{li2025reinforcement,
  title={Reinforcement learning outperforms supervised fine-tuning: A case study on audio question answering},
  author={Li, Gang and Liu, Jizhong and Dinkel, Heinrich and Niu, Yadong and Zhang, Junbo and Luan, Jian},
  journal={arXiv preprint arXiv:2503.11197},
  year={2025}
}
@article{wen2025sari,
  title={SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning},
  author={Wen, Cheng and Guo, Tingwei and Zhao, Shuaijiang and Zou, Wei and Li, Xiangang},
  journal={arXiv preprint arXiv:2504.15900},
  year={2025}
}
@article{busso2008iemocap,
  title={IEMOCAP: Interactive emotional dyadic motion capture database},
  author={Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S},
  journal={Language resources and evaluation},
  volume={42},
  pages={335--359},
  year={2008},
  publisher={Springer}
}
@article{poria2018meld,
  title={Meld: A multimodal multi-party dataset for emotion recognition in conversations},
  author={Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Naik, Gautam and Cambria, Erik and Mihalcea, Rada},
  journal={arXiv preprint arXiv:1810.02508},
  year={2018}
}
@article{livingstone2018ryerson,
  title={The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
  author={Livingstone, Steven R and Russo, Frank A},
  journal={PloS one},
  volume={13},
  number={5},
  pages={e0196391},
  year={2018},
  publisher={Public Library of Science San Francisco, CA USA}
}
@article{jackson2014surrey,
  title={Surrey audio-visual expressed emotion (savee) database},
  author={Jackson, Philip and Haq, SJUoSG},
  journal={University of Surrey: Guildford, UK},
  year={2014}
}
@article{alsabhan2023human,
  title={Human--computer interaction with a real-time speech emotion recognition with ensembling techniques 1D convolution neural network and attention},
  author={Alsabhan, Waleed},
  journal={Sensors},
  volume={23},
  number={3},
  pages={1386},
  year={2023},
  publisher={MDPI}
}
@inproceedings{madanian2022automatic,
  title={Automatic speech emotion recognition using machine learning: digital transformation of mental health},
  author={Madanian, Samaneh and Parry, David and Adeleye, Olayinka and Poellabauer, Christian and Mirza, Farhaan and Mathew, Shilpa and Schneider, Sandy},
  booktitle={Proceedings of the Annual Pacific Asia Conference on Information Systems (PACIS)},
  year={2022}
}
@inproceedings{li2021speech,
  title={Speech emotion recognition for power customer service},
  author={Li, Xutong and Lin, Rongheng},
  booktitle={2021 7th International Conference on Computer and Communications (ICCC)},
  pages={514--518},
  year={2021},
  organization={IEEE}
}
@article{ma2023emotion2vec,
  title={emotion2vec: Self-supervised pre-training for speech emotion representation},
  author={Ma, Ziyang and Zheng, Zhisheng and Ye, Jiaxin and Li, Jinchao and Gao, Zhifu and Zhang, Shiliang and Chen, Xie},
  journal={arXiv preprint arXiv:2312.15185},
  year={2023}
}
@article{chen2024vesper,
  title={Vesper: A compact and effective pretrained model for speech emotion recognition},
  author={Chen, Weidong and Xing, Xiaofen and Chen, Peihao and Xu, Xiangmin},
  journal={IEEE Transactions on Affective Computing},
  year={2024},
  publisher={IEEE}
}
@inproceedings{wang2025enabling,
  title={Enabling auditory large language models for automatic speech quality evaluation},
  author={Wang, Siyin and Yu, Wenyi and Yang, Yudong and Tang, Changli and Li, Yixuan and Zhuang, Jimin and Chen, Xianzhao and Tian, Xiaohai and Zhang, Jun and Sun, Guangzhi and others},
  booktitle={ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2025},
  organization={IEEE}
}
@misc{waheed2024speech,
      title={What Do Speech Foundation Models Not Learn About Speech?}, 
      author={Abdul Waheed and Hanin Atwany and Bhiksha Raj and Rita Singh},
      year={2024},
      eprint={2410.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.12948}, 
}
@inproceedings{ye2023temporal,
  title={Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition},
  author={Ye, Jiaxin and Wen, Xin-Cheng and Wei, Yujie and Xu, Yong and Liu, Kunhong and Shan, Hongming},
  booktitle={ICASSP 2023-2023 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}
@article{ma2024emobox,
  title={Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark},
  author={Ma, Ziyang and Chen, Mingjie and Zhang, Hezhao and Zheng, Zhisheng and Chen, Wenxi and Li, Xiquan and Ye, Jiaxin and Chen, Xie and Hain, Thomas},
  journal={arXiv preprint arXiv:2406.07162},
  year={2024}
}